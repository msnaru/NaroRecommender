{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TestScrapingNaro.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/msnaru/NaroRecommender/blob/master/TestScrapingNaro.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "jzX3ZVbXB5zD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "db860b19-0386-4f9e-b383-c5d3598aeb9a"
      },
      "cell_type": "code",
      "source": [
        "# モジュールのインストール\n",
        "\n",
        "!pip install BeautifulSoup4\n",
        "!pip install lxml"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: BeautifulSoup4 in /usr/local/lib/python3.6/dist-packages (4.6.3)\n",
            "Collecting lxml\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/a4/9eea8035fc7c7670e5eab97f34ff2ef0ddd78a491bf96df5accedb0e63f5/lxml-4.2.5-cp36-cp36m-manylinux1_x86_64.whl (5.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.8MB 4.9MB/s \n",
            "\u001b[?25hInstalling collected packages: lxml\n",
            "Successfully installed lxml-4.2.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "at2k_lzfPwci",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# モジュールのimport\n",
        "\n",
        "import pandas as pd\n",
        "from time import sleep\n",
        "import urllib.request\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MfDNQ0nstV0K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 関数定義\n",
        "\n",
        "\n",
        "# 感想の最大ページ数抽出\n",
        "def MaxPage(url):\n",
        "  # 実際の最大ページ数を超えるページにアクセスすると、実際の最大ページにアクセスされる\n",
        "  url += \"?p=10000\"\n",
        "  \n",
        "  try:\n",
        "    xml = urllib.request.urlopen(url)\n",
        "    soup = BeautifulSoup(xml, \"lxml\")\n",
        "    naviall = soup.find_all(\"div\", class_ = 'naviall')\n",
        "    if len(naviall) > 0:\n",
        "      strNaviall = repr(naviall[1].text.split(\" \")[1]).split(\"\\\\xa\")\n",
        "      nMaxPpage = int(strNaviall[len(strNaviall) - 1][:-1])\n",
        "      return nMaxPpage\n",
        "    else:\n",
        "      return 1\n",
        "  \n",
        "  except:\n",
        "    print(\"You cannot get a number of page.\")\n",
        "        \n",
        "      \n",
        "# 感想スクレイピング処理\n",
        "def scraping(url, lenURL, i):\n",
        "    xml = urllib.request.urlopen(url)\n",
        "    soup = BeautifulSoup(xml, \"lxml\")\n",
        "    comments = soup.find_all(\"div\", class_='comment')\n",
        "    \n",
        "    df_add = pd.DataFrame(columns=[\"impression\"])\n",
        "    for comment in comments:\n",
        "        comment = comment.text\n",
        "        df_add = df_add.append(pd.DataFrame([[comment]], columns=[\"impression\"]))[df_add.columns.tolist()]\n",
        "    \n",
        "\n",
        "    next_url = url[:lenURL] + \"?p=\" + str(i+2)\n",
        "    \n",
        "    return df_add,next_url\n",
        "  \n",
        "  \n",
        "# 感想抽出処理\n",
        "def Extract_imp(url):\n",
        "\n",
        "  lenURL = len(url)\n",
        "\n",
        "  # 取得する感想のページ数。最大ページ数を制限\n",
        "  nMaxPage = MaxPage(url)\n",
        "  if nMaxPage > 100:\n",
        "    nMaxPage = 100\n",
        "\n",
        "  df = pd.DataFrame(columns=[\"impression\"])\n",
        "\n",
        "  # scraping(url) を呼び出してスクレイピングを実行\n",
        "  for i in range(nMaxPage):\n",
        "    df_add, url= scraping(url, lenURL, i)\n",
        "    df = df.append(df_add,ignore_index=True)\n",
        "    sleep(1)\n",
        "\n",
        "  # スクレイピングしたデータに重複データがあることを確認\n",
        "  print(\"データの個数：\"+str(len(df)))\n",
        "\n",
        "  #データに重複があるものは全て削除\n",
        "  df_cleansing = df.drop_duplicates(keep=False)\n",
        "  print(\"データの個数：\"+str(len(df_cleansing)))\n",
        "  \n",
        "  return [df, df_cleansing]\n",
        "\n",
        "\n",
        "# 自分の評価済み小説のタイトルと評価のリストを取得\n",
        "def Evaluated_title():\n",
        "\n",
        "  url = \"https://mypage.syosetu.com/mypagenovelhyoka/list/userid/63023/\"\n",
        "  xml = urllib.request.urlopen(url)\n",
        "  soup = BeautifulSoup(xml, \"lxml\")\n",
        "  titles = soup.find_all(\"li\", class_ ='title')\n",
        "  hyokas = soup.find_all(\"p\", class_ ='hyouka')\n",
        "\n",
        "\n",
        "  title_list = []\n",
        "\n",
        "  for title in titles:\n",
        "      title = repr(title.text)\n",
        "      title_list.append((title[:-4]).split(\"(\")[1])\n",
        "\n",
        "  pd_title = pd.DataFrame({'title': title_list})\n",
        "  \n",
        "\n",
        "  i = 0\n",
        "  hyoka_pair_list = []\n",
        "  hyoka_pair = []\n",
        "\n",
        "  for hyoka in hyokas:\n",
        "    hyoka = repr(hyoka.text)\n",
        "    i += 1\n",
        "    if i % 2 == 0:\n",
        "      hyoka_pair.append(int(hyoka[-4:][:1]))\n",
        "      hyoka_pair_list.append(hyoka_pair)\n",
        "      hyoka_pair = []\n",
        "    else:\n",
        "      hyoka_pair.append(int(hyoka[-4:][:1]))\n",
        "\n",
        "      \n",
        "  pd_hyoka = pd.DataFrame(hyoka_pair_list)\n",
        "  pd_hyoka = pd_hyoka.rename(columns={0: 'ev_story', 1: 'ev_sentence'})\n",
        "  \n",
        "  pd_Evaluated = pd.concat([pd_title, pd_hyoka], axis=1)\n",
        "  return pd_Evaluated\n",
        "\n",
        "\n",
        "# 任意のタイトルのトップリンクと感想リンクの抽出\n",
        "def links(title):\n",
        "\n",
        "  link_title = \"https://ncode.syosetu.com/\" + title + \"/\"\n",
        "\n",
        "\n",
        "  xml = urllib.request.urlopen(link_title)\n",
        "  soup = BeautifulSoup(xml, \"lxml\")\n",
        "  comments = soup.find_all(\"div\", id='novel_header')\n",
        "\n",
        "  l = str(comments).splitlines()\n",
        "\n",
        "  l_in = [s for s in l if 'https://novelcom.syosetu.com/impression/list/ncode/' in s]\n",
        "  link_imp = l_in[0].split('\"')[1]\n",
        "\n",
        "\n",
        "  return [link_title, link_imp]\n",
        "\n",
        "\n",
        "def add_links(df):\n",
        "  try:\n",
        "    pd_links = pd.DataFrame([links(title) for title in df.loc[:,\"title\"].tolist()])\n",
        "    pd_links = pd_links.rename(columns={0: 'link_title', 1: 'link_impression'})\n",
        "\n",
        "    return pd.concat([df, pd_links], axis = 1)\n",
        "  \n",
        "  except:\n",
        "    print(\"入力DataFrameが正しくありません\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JLBxM8iO8k4H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "52c98bd0-1ab9-489c-b230-10ce6ab04f87"
      },
      "cell_type": "code",
      "source": [
        "df = Evaluated_title()\n",
        "df2 = add_links(df)\n",
        "\n",
        "impressions = [Extract_imp(url) for url in df2.loc[:, \"link_impression\"].tolist()]\n",
        "\n",
        "for i in range(10):\n",
        "  #CSVでデータを保存\n",
        "  impressions[i][0].to_csv(\"impression%d.csv\"% i, index=False, encoding=\"UTF-8\")\n",
        "  impressions[i][1].to_csv(\"impression%d_cleansed.csv\"% i, index=False, encoding=\"UTF-8\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "データの個数：1549\n",
            "データの個数：1503\n",
            "データの個数：219\n",
            "データの個数：148\n",
            "データの個数：1078\n",
            "データの個数：1038\n",
            "データの個数：337\n",
            "データの個数：318\n",
            "データの個数：22\n",
            "データの個数：22\n",
            "データの個数：6\n",
            "データの個数：6\n",
            "データの個数：1325\n",
            "データの個数：1311\n",
            "データの個数：2214\n",
            "データの個数：2214\n",
            "データの個数：492\n",
            "データの個数：484\n",
            "データの個数：1111\n",
            "データの個数：1111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ShkPwNCENHQH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E1UuOzttDqwU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}